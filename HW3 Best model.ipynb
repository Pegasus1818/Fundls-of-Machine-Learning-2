{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177a0d9b",
   "metadata": {},
   "source": [
    "# Text Sentiment Classification By LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ded785",
   "metadata": {},
   "source": [
    "### Task Allocation\n",
    "#### WU XIAOQING: Att_BiLSTM, Semi-supervised learning, parameter adjustment\n",
    "#### LI YISHAN: schedular,parameter adjustment\n",
    "#### ZHENG DAYI:   free rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dd90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "path_prefix = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813eece",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2109658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_training_data(path='Train_label.txt'):\n",
    "    # Read training data\n",
    "    if 'Train_label' in path:\n",
    "        with open(path, 'r',encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "        x = [line[2:] for line in lines]\n",
    "        y = [line[0] for line in lines]\n",
    "        return x, y\n",
    "    else:\n",
    "        with open(path, 'r',encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "\n",
    "def load_testing_data(path='Test.txt'):\n",
    "    # Read testing data\n",
    "    with open(path, 'r',encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
    "        X = [sen.split(' ') for sen in X]\n",
    "    return X\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    #outputs => probability (float)\n",
    "    #labels => labels\n",
    "    outputs[outputs>=0.5] = 1 # Negtive Sentiment\n",
    "    outputs[outputs<0.5] = 0 # Positive Sentiment\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319094e6",
   "metadata": {},
   "source": [
    "## Train Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb723cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading testing data ...\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def train_word2vec(x):\n",
    "    model = word2vec.Word2Vec(x, vector_size=250, window=5, min_count=5, workers=12)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"loading training data ...\")\n",
    "    train_x, y = load_training_data('Train_label.txt')\n",
    "    train_x_no_label = load_training_data('Train_nolabel.txt')\n",
    "\n",
    "    print(\"loading testing data ...\")\n",
    "    test_x = load_testing_data('Test.txt')\n",
    "\n",
    "    model = train_word2vec(train_x + train_x_no_label + test_x)\n",
    "    \n",
    "    print(\"saving model ...\")\n",
    "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb8f66",
   "metadata": {},
   "source": [
    "## Data Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14a92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "    def get_w2v_model(self):\n",
    "        # load word to vector model\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "    def add_embedding(self, word):\n",
    "        # add word into embedding\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for i, word in enumerate(self.embedding.wv.key_to_index):\n",
    "            print('get words #{}'.format(i+1), end='\\r')\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding.wv[word])\n",
    "        print('')\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "    def pad_sequence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    def sentence_word2idx(self):\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    def labels_to_tensor(self, y):\n",
    "        # turn labels into tensors\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681805af",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e94b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class TwitterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    Data can be a list of numpy array or a list of lists\n",
    "    input data shape : (data_num, seq_len, feature_dim)\n",
    "    \n",
    "    __len__ will return the number of data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edad2d4",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c331d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # Whether fix embedding\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                          nn.Linear(hidden_dim, 64),\n",
    "                          nn.Dropout(dropout),\n",
    "                          nn.Linear(64, 1),\n",
    "                          nn.Sigmoid() )\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # dimension of x (batch, seq_len, hidden_size)\n",
    "        x = x[:, -1, :] \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51678d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atten_BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(Atten_BiLSTM, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # Whether the embedding is fixed. If the fix_embedding is False, the embedding will be trained\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                                        nn.Linear(hidden_dim, 128),\n",
    "                                        nn.Dropout(dropout),\n",
    "                                        nn.Linear(128, 64),\n",
    "                                        nn.Dropout(dropout),\n",
    "                                        nn.Linear(64, 16),\n",
    "                                        nn.Dropout(dropout),\n",
    "                                        nn.Linear(16, 1),\n",
    "                                        nn.Sigmoid())\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def attention(self, output, hidden):\n",
    "        # output  (batch_size, seq_len, hidden_size * num_direction)\n",
    "        # hidden (batch_size, num_layers * num_direction, hidden_size)\n",
    "\n",
    "        output = output[:,:,:self.hidden_dim] + output[:,:,self.hidden_dim:] # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "        hidden = hidden.unsqueeze(1) # (batch_size, 1, hidden_size)\n",
    "\n",
    "        atten_w = self.attention_layer(hidden) # (batch_size, 1, hidden_size)\n",
    "        m = nn.Tanh()(output) # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "\n",
    "        context = torch.bmm(softmax_w, output)\n",
    "\n",
    "        return context.squeeze(1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "\n",
    "        # x (batch, seq_len, hidden_size)\n",
    "        # hidden (num_layers *num_direction, batch_size, hidden_size)\n",
    "        x, (hidden, _) = self.lstm(inputs, None)\n",
    "        hidden = hidden.permute(1, 0, 2) # (batch_size, num_layers *num_direction, hidden_size)\n",
    "\n",
    "        # atten_out [batch_size, 1, hidden_dim]\n",
    "        atten_out = self.attention(x, hidden)\n",
    "        return self.classifier(atten_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224650df",
   "metadata": {},
   "source": [
    "## Define Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98882e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset,Subset,DataLoader\n",
    "\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    model.train() # set training mode\n",
    "    criterion = nn.BCELoss() # Define loss function\n",
    "    t_batch = len(train) \n",
    "    v_batch = len(valid) \n",
    "    training=train\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # set optimizer as SGD (you can change it)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        model.train()\n",
    "        # For training\n",
    "        t_batch = len(training)\n",
    "        for i, (inputs, labels) in enumerate(training): \n",
    "            inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n",
    "            labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            correct = evaluation(outputs, labels) # calculate accuracy\n",
    "            total_acc += (correct / len(training))\n",
    "            total_loss += loss.item()\n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "\n",
    "        # For validation\n",
    "        model.eval() # set validation mode\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n",
    "                labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                correct = evaluation(outputs, labels)\n",
    "                total_acc += (correct / batch_size)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "            scheduler.step(total_acc)\n",
    "            if total_acc > best_acc:\n",
    "                # if the result of validation is better than previous model, save the new model\n",
    "                best_acc = total_acc\n",
    "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
    "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "                #### add Semi-supervised learning\n",
    "                if (total_acc/v_batch > 0.8):\n",
    "                    dataset = get_pseudo_labels(semi_dataset, model)\n",
    "                    training = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False,drop_last=True)\n",
    "                    print(\"update semi_dataset\")\n",
    "        print('-----------------------------------------------')\n",
    "        \n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16554673",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80182537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            ret_output += outputs.int().tolist()\n",
    "    \n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "305bb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(outputs, threshold=0.99):\n",
    "    idx = (outputs>=threshold) | (outputs<1-threshold)\n",
    "    outputs[outputs>=threshold] = 1 # 大于等于 threshold 为正面\n",
    "    outputs[outputs<1-threshold] = 0 # 小于 threshold 为负面\n",
    "    return outputs.long(), idx\n",
    "\n",
    "def get_pseudo_labels(dataset, model):\n",
    "    # This functions generates pseudo-labels of a dataset using given model.\n",
    "    # It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n",
    "    # You are NOT allowed to use any models trained on external data for pseudo-labeling.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    data_loader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, shuffle = True, num_workers = 0)    \n",
    "    # Make sure the model is in eval mode.\n",
    "    model.eval()\n",
    "    # Define softmax function.\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    idx = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the dataset by batches.\n",
    "    for i, (inputs) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            label, idd = add_label(outputs)\n",
    "            for j,x in enumerate(outputs):\n",
    "                if idd[j]==True:\n",
    "                    idx.append(i * batch_size + j)\n",
    "                    labels.append(int(label[j]))\n",
    "    dataset = TwitterDataset(Subset(dataset, idx), torch.LongTensor(labels))\n",
    "    dataset = ConcatDataset([train_dataset, dataset]) \n",
    "          ### merge new set with training set\n",
    "    # # Turn off the eval mode.\n",
    "    model.train()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b3284",
   "metadata": {},
   "source": [
    "## Parameter setting + Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8d7414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #23000\n",
      "total words: 23002\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #23000\n",
      "total words: 23002\n",
      "sentence count #149882\n",
      "start training, parameter total:11543041, trainable:5792541\n",
      "\n",
      "[ Epoch1: 508/508 ] loss:0.441 acc:67.578 \n",
      "Train | Loss:0.49796 Acc: 38.534\n",
      "Valid | Loss:0.44233 Acc: 78.826 \n",
      "saving model with acc 78.826\n",
      "-----------------------------------------------\n",
      "[ Epoch2: 508/508 ] loss:0.424 acc:66.406 \n",
      "Train | Loss:0.43706 Acc: 40.493\n",
      "Valid | Loss:0.42070 Acc: 80.544 \n",
      "saving model with acc 80.544\n",
      "-----------------------------------------------\n",
      "[ Epoch3: 508/508 ] loss:0.406 acc:66.797 \n",
      "Train | Loss:0.40573 Acc: 41.427\n",
      "Valid | Loss:0.41683 Acc: 80.834 \n",
      "saving model with acc 80.834\n",
      "-----------------------------------------------\n",
      "[ Epoch4: 508/508 ] loss:0.435 acc:64.453 \n",
      "Train | Loss:0.37504 Acc: 42.221\n",
      "Valid | Loss:0.41981 Acc: 80.869 \n",
      "saving model with acc 80.869\n",
      "-----------------------------------------------\n",
      "[ Epoch5: 508/508 ] loss:0.362 acc:68.359 \n",
      "Train | Loss:0.33674 Acc: 43.213\n",
      "Valid | Loss:0.43134 Acc: 80.995 \n",
      "saving model with acc 80.995\n",
      "-----------------------------------------------\n",
      "[ Epoch6: 508/508 ] loss:0.224 acc:72.656 \n",
      "Train | Loss:0.28953 Acc: 44.382\n",
      "Valid | Loss:0.51771 Acc: 80.874 \n",
      "-----------------------------------------------\n",
      "[ Epoch7: 508/508 ] loss:0.188 acc:74.609 \n",
      "Train | Loss:0.23267 Acc: 45.665\n",
      "Valid | Loss:0.59401 Acc: 79.682 \n",
      "-----------------------------------------------\n",
      "[ Epoch8: 508/508 ] loss:0.243 acc:74.219 \n",
      "Train | Loss:0.17543 Acc: 46.956\n",
      "Valid | Loss:0.69068 Acc: 79.472 \n",
      "-----------------------------------------------\n",
      "[ Epoch9: 508/508 ] loss:0.109 acc:78.516 \n",
      "Train | Loss:0.12986 Acc: 47.946\n",
      "Valid | Loss:0.80208 Acc: 79.036 \n",
      "-----------------------------------------------\n",
      "[ Epoch10: 508/508 ] loss:0.064 acc:79.688 \n",
      "Train | Loss:0.10048 Acc: 48.555\n",
      "Valid | Loss:1.15522 Acc: 78.786 \n",
      "-----------------------------------------------\n",
      "[ Epoch11: 508/508 ] loss:0.132 acc:78.516 \n",
      "Train | Loss:0.08048 Acc: 48.894\n",
      "Valid | Loss:1.10399 Acc: 78.180 \n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "-----------------------------------------------\n",
      "[ Epoch12: 508/508 ] loss:0.003 acc:81.250  \n",
      "Train | Loss:0.03742 Acc: 49.739\n",
      "Valid | Loss:2.55979 Acc: 79.227 \n",
      "-----------------------------------------------\n",
      "[ Epoch13: 508/508 ] loss:0.014 acc:80.469  \n",
      "Train | Loss:0.01790 Acc: 50.071\n",
      "Valid | Loss:4.37261 Acc: 78.991 \n",
      "-----------------------------------------------\n",
      "[ Epoch14: 508/508 ] loss:0.027 acc:80.469  \n",
      "Train | Loss:0.01337 Acc: 50.128\n",
      "Valid | Loss:5.09279 Acc: 78.966 \n",
      "-----------------------------------------------\n",
      "[ Epoch15: 508/508 ] loss:0.002 acc:81.250  \n",
      "Train | Loss:0.01060 Acc: 50.174\n",
      "Valid | Loss:5.85754 Acc: 78.941 \n",
      "-----------------------------------------------\n",
      "[ Epoch16: 508/508 ] loss:0.020 acc:80.469  \n",
      "Train | Loss:0.00936 Acc: 50.196\n",
      "Valid | Loss:6.33923 Acc: 78.691 \n",
      "-----------------------------------------------\n",
      "[ Epoch17: 508/508 ] loss:0.002 acc:81.250  \n",
      "Train | Loss:0.00815 Acc: 50.218\n",
      "Valid | Loss:6.77730 Acc: 78.871 \n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
      "-----------------------------------------------\n",
      "[ Epoch18: 508/508 ] loss:0.008 acc:80.859  \n",
      "Train | Loss:0.00664 Acc: 50.249\n",
      "Valid | Loss:7.03736 Acc: 78.916 \n",
      "-----------------------------------------------\n",
      "[ Epoch19: 508/508 ] loss:0.000 acc:81.250  \n",
      "Train | Loss:0.00630 Acc: 50.254\n",
      "Valid | Loss:7.27817 Acc: 78.966 \n",
      "-----------------------------------------------\n",
      "[ Epoch20: 508/508 ] loss:0.019 acc:80.469  \n",
      "Train | Loss:0.00603 Acc: 50.264\n",
      "Valid | Loss:7.46977 Acc: 78.861 \n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set data path\n",
    "train_with_label = os.path.join(path_prefix, 'Train_label.txt')\n",
    "train_no_label = os.path.join(path_prefix, 'Train_nolabel.txt')\n",
    "testing_data = os.path.join(path_prefix, 'Test.txt')\n",
    "w2v_path = os.path.join(path_prefix, 'w2v_all.model')\n",
    "\n",
    "\n",
    "sen_len = 30\n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 256\n",
    "epoch = 20\n",
    "lr = 0.001\n",
    "model_dir = path_prefix\n",
    "\n",
    "print(\"loading data ...\")\n",
    "train_x, y = load_training_data(train_with_label)\n",
    "train_x_no_label = load_training_data(train_no_label)\n",
    "\n",
    "# Preprocessing\n",
    "preprocess1 = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
    "preprocess2 = Preprocess(train_x_no_label, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess1.make_embedding(load=True)\n",
    "embedding = preprocess2.make_embedding(load=True)\n",
    "train_x = preprocess1.sentence_word2idx()\n",
    "y = preprocess1.labels_to_tensor(y)\n",
    "train_x_no_label = preprocess2.sentence_word2idx()\n",
    "semi_dataset = TwitterDataset(X=train_x_no_label, y=None)\n",
    "\n",
    "#model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=250, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "model=Atten_BiLSTM(embedding, embedding_dim=250, hidden_dim=300, num_layers=3)\n",
    "model = model.to(device) \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_x[:130000], train_x[130000:], y[:130000], y[130000:]\n",
    "\n",
    "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
    "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
    "\n",
    "# transfor data into batch of tensors\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "# Begin Training\n",
    "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1902a3bb",
   "metadata": {},
   "source": [
    "## Predict and save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5442912b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #23000\n",
      "total words: 23002\n",
      "sentence count #49800\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data(testing_data)\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "print('\\nload model ...')#Predict-and-save-to-csv-file\n",
    "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "# save as csv\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"labels\":outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
    "print(\"Finish Predicting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d2272",
   "metadata": {},
   "source": [
    "# Hint\n",
    "* Optimizer\n",
    "* learning rate\n",
    "* epoch\n",
    "* batch size\n",
    "* Activation function\n",
    "* Self-Training for unlabel training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07286bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
